import torch
from torchvision import datasets, transforms
import torch.nn as nn

"""
Overall, these lines define the architecture of the CNN model, including its convolutional and fully connected layers, specifying how the input data is transformed through the network to produce the final output.
"""
"""
NOTE: ANALOGY FOR conv1 and conv2
conv1 (First Convolutional Layer): Imagine you have a security system at the entrance of a building. The system has several different types of sensors, each looking for specific features, like the shape of a person's face, the color of their clothing, or whether they are carrying something. These sensors work together to detect and extract basic features from people entering the building.

conv2 (Second Convolutional Layer): Now, imagine there's a second layer of sensors inside the building, which is more specialized. These sensors analyze the features detected by the first layer to identify more complex patterns, such as specific outfits (like a uniform) or behaviors (like carrying a briefcase). This second layer of sensors provides a deeper understanding of the people entering the building.

In this analogy, the first convolutional layer (conv1) acts like the initial security check, detecting basic features, while the second convolutional layer (conv2) analyzes the results from the first layer to identify more complex patterns. Together, these layers help the security system understand and classify the people entering the building.

More concrete overview:

In a Convolutional Neural Network (CNN), conv1 and conv2 are typically used to refer to the first and second convolutional layers, respectively. These layers are responsible for extracting features from the input image.

Here's a brief explanation of the role of each layer:

conv1 (First Convolutional Layer): This is the initial layer in the CNN. It applies a set of filters (also known as kernels) to the input image, performing convolution operations. Each filter detects specific features, such as edges or textures, in different parts of the image. The output of this layer is a set of feature maps, each representing the presence of a particular feature in the input image.

conv2 (Second Convolutional Layer): This layer follows the first convolutional layer. It applies another set of filters to the feature maps generated by the previous layer. The purpose of this layer is to further extract higher-level features by combining and refining the features detected in the earlier layers. The output of conv2 is a new set of feature maps that capture more abstract and complex patterns in the input image.

In summary, conv1 and conv2 are essential components of a CNN, working together to extract increasingly complex features from the input image, which are then used for subsequent tasks such as classification or object detection.
"""
class ASLModel(nn.Module):
    def __init__(self, num_classes=26): # 26 alphabets in ASL language
        super(ASLModel, self).__init__() #This line calls the constructor of the parent class (nn.Module) to initialize the ASLCNN class. This is necessary to properly set up the model.

         # Define the image preprocessing steps (resize and conversion to tensor)
        self.preprocessor = transforms.Compose([
            transforms.Resize((244, 244)), # a common resizing size for images in machine learning tasks, particularly for convolutional neural networks (CNNs), is 224x224 pixels. 
            transforms.ToTensor() # convert images to pytorch tensors | neural networks operate on tensors, so this transformation is necessary to convert the image data into a format that the network can process.
        ])



        """
        This line defines the first convolutional layer (conv1) of the CNN. Here's what each part means:

        nn.Conv2d: This is a class in PyTorch's nn module that represents a 2-dimensional convolutional layer. Convolutional layers are used to extract features from input data.

        (3, 32, 3, 1): These are the parameters passed to the Conv2d constructor:

        3: The number of input channels. In this case, it's 3 because RGB images have 3 color channels (red, green, blue).
        32: The number of output channels or filters. This determines the depth of the output volume. Each filter detects different features in the input.
        3: The size of the convolutional kernel (filter) is 3x3. This specifies the spatial dimensions of the filter.
        1: The stride of the convolution. It specifies the step size of the filter moving across the input. Here, it moves 1 pixel at a time.

        ==== NOTE: EXPLAINING KERNEL_SIZE AND STRIDE ====
        Kernel Size (kernel_size):

        In a CNN, the kernel (or filter) is a small matrix used for the convolution operation.
        kernel_size=3 means the kernel will be a 3x3 matrix. This 3x3 kernel will slide (or convolve) across the input image in both the horizontal and vertical directions.

        The size of the kernel determines the size of the feature map produced after convolution. A larger kernel can capture more complex patterns but may also lead to more parameters and computation.

        NOTE: Why 3x3?
        Effective Feature Extraction: A 3x3 kernel can capture local patterns and features in an image, such as edges, corners, and textures.

        Computational Efficiency: A 3x3 kernel strikes a balance between capturing useful features and computational efficiency. It is small enough to be computationally feasible yet large enough to capture meaningful patterns.

        Hierarchical Feature Learning: CNNs typically consist of multiple layers, each performing convolution with 3x3 kernels. This hierarchical approach allows the network to learn increasingly complex features as information flows through the layers.

        Stride (stride):

        Stride refers to the number of pixels by which the kernel is shifted over the input image.
        stride=1 means the kernel moves one pixel at a time. This is the most common setting and is often used to preserve spatial resolution.
        A larger stride value would result in a smaller output size because the kernel would move more quickly across the input, skipping over pixels.
        """
         # Convolutional layer 1: 3 input channels (RGB), 16 output channels, 3x3 kernel size
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)


        """
        self.conv2 = nn.Conv2d(16, 32, 3, 1):
        ==== NOTE: EXPLAINING 32 ====
        This is similar to the first convolutional layer but takes 16 input channels (output from the first layer) and produces 32 output channels. This layer learns more complex features compared to the first layer.

        NOT USING 64 ONLY 32!
        64: The choice of 64 filters in the nn.Conv2d(16, 32, 3, 1) statement is a common practice in CNN design, but it's not arbitrary. Here's why:

        Increasing Complexity: Each filter in a convolutional layer learns to detect a specific pattern or feature in the input. By using more filters, the network can learn to detect a wider range of features, increasing its ability to understand complex patterns in the data.

        Hierarchical Feature Learning: CNNs typically have multiple layers, each building upon the features learned by the previous layers. Having more filters in a layer allows the network to learn more abstract and high-level features as it progresses through the layers.

        Historical Precedence: Over the years, researchers have found that using powers of 2 (like 64) for the number of filters can be computationally efficient, especially on hardware that is optimized for such sizes.

       The choice of 32 output channels in the second convolutional layer (conv2) is somewhat arbitrary and can be considered a hyperparameter. In convolutional neural networks (CNNs), the number of output channels is typically chosen based on the complexity of the features you want the layer to learn and the overall network architecture.

        In this case, increasing the number of output channels in conv2 from 16 (the number of channels in the output of conv1) to 32 allows the network to learn more diverse and complex features in the higher layers of the network. However, the specific choice of 32 channels is often determined through experimentation and tuning to achieve the best performance on a given task.

        In summary, while the choice of 32 output channels in conv2 is not arbitrary and is based on the desired complexity of the network, it is a hyperparameter that can be adjusted based on the specific requirements of your model and the dataset.
        """

        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)


        """
        NOTE: What is max pool?

        The point of the max pooling layer in a convolutional neural network (CNN) like the one used for the ASL interpreter is to downsample the feature maps produced by the convolutional layers. This downsampling helps in several ways:

        Reduces Computational Complexity: By reducing the spatial dimensions of the feature maps, the number of parameters and computations in the network are reduced, making it more computationally efficient.

        Increases Robustness: Max pooling helps in making the network more robust to small changes in the input images. It does this by abstracting over the spatial details and focusing on the most important features in a local neighborhood.

        Hierarchical Feature Learning: Max pooling is often used in conjunction with convolutional layers to create a hierarchy of features. The convolutional layers extract low-level features (such as edges and textures), and the max pooling layers combine these features to form higher-level representations.

        In the case of the ASL interpreter, the max pooling layer with a kernel size of 2x2 and a stride of 2 reduces the spatial dimensions of the feature maps by a factor of 2 in both height and width. This downsampling helps in capturing the most important features in the input images while reducing the computational load of the network.
===
        The placement of the pooling layers (pool) in a convolutional neural network (CNN) can impact the network's performance and behavior. Generally, it's common to place pooling layers after convolutional layers for several reasons:

        Reduce Spatial Dimensions: Pooling layers help in reducing the spatial dimensions of the feature maps, which can help in reducing the computational complexity of the network and controlling overfitting.

        Abstracting Features: By downsampling the feature maps, pooling layers help in abstracting the learned features, focusing on the most important information in the feature maps.

        Capture Invariance: Pooling helps the network become more invariant to small translations or distortions in the input, making the learned features more robust.

        Placing the pooling layers after the convolutional layers allows the network to capture spatial hierarchies of features. The convolutional layers extract low-level features, and the pooling layers combine these features to form higher-level representations.
===
        Max Pooling Layer Parameters (self.pool):

        kernel_size=2: The max pooling layer uses a 2x2 kernel to pool over the feature maps produced by the convolutional layer. This means it takes the maximum value from a 2x2 window.

        stride=2: The stride of 2 means that the pooling operation moves the 2x2 window 2 pixels at a time horizontally and vertically. This reduces the spatial dimensions of the feature maps by a factor of 2.
        padding=0: No padding is used in the max pooling layer. This means that the pooling operation is applied only to the valid part of the input, and no padding is added to the feature maps.

        In summary, the convolutional layer uses a larger kernel size, stride of 1, and padding to extract features from the input image, while the max pooling layer uses a smaller kernel size, stride of 2, and no padding to downsample the feature maps and reduce their spatial dimensions.
        """
        # Max pooling layer 1: 2x2 kernel size, stride of 2
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

        """
        Fully Connected Layers (FC Layers):

        These layers come after the convolutional layers (conv1 and conv2) and are responsible for classifying the extracted features into the 26 ASL alphabet classes. Unlike convolutional layers that work with spatial data, FC layers treat the input as a one-dimensional vector. 

        For in_feature and out_features, here's how you would determine the paratmers:

        After the first convolutional layer (conv1), the spatial dimensions remain the same due to padding. So, the output feature map size is 244x244.

        When we apply max pooling with a kernel size of 2x2 and a stride of 2, the pooling operation looks at each 2x2 square of pixels in the input feature map and selects the maximum value. By moving the pooling window 2 pixels at a time, the output feature map size is halved in each dimension.

        Therefore, after the max pooling layer, the spatial dimensions are reduced to half, resulting in a feature map size of 122x122.
        """

        self.fc1 = nn.Linear(32* 122 * 122, 128)

        """
        self.fc2 = nn.Linear(128, 26):
        This defines the second fully connected layer (fc2).
        The input size is 128, which matches the output size of the previous layer fc1.
        The output size is 26, which corresponds to the number of classes in the ASL alphabet. Each neuron in this layer represents a class, and the output of the network will be a probability distribution over these 26 classes.
        The purpose of this layer is to take the features learned by the previous layers (fc1) and map them to the output classes, making the final predictions for the ASL alphabet classification task.
        """
        self.fc2 = nn.Linear(128, out_features=num_classes)  # 26 classes for ASL alphabet
